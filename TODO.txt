################################################################################
# General
################################################################################

- Consider moving to https://github.com/tomz/liblinear-ruby-swig. May crash less
  than https://github.com/fcheung/rubylinear

- It might be cleaner to not represent Rests explicitly, but rather to let notes
  have an IOI (which would include the rest's duration) as well as a duration.

################################################################################
# Phrase detection
################################################################################

- It still surprises the hell out of of me that this isn't TRIVIAL. "My Bonnie 
  lies over the ocean" is practically verbatim AAAA BA'BA'.  Why isn't that a
  trivial win? Go back to printing out the details.

- A very different (and more cognitively plausible) approach would be to go 
  left-to-right, building up a phrase structure as we go. This would have the 
  advantage of weighting the initial material, which is almost always the most 
  important phrase. It would avoid randomly walking into weird phrase-structure
  space and then trying to course-correct back to something sensible.

- What we're getting from the phrase detection isn't sufficient to do 
  composition in the way I was expecting. It doesn't draw correspondences 
  between phrases. If we were to track correspondences, we could use that to 
  perform similar operations on groups of phrases.  E.g., if two phrases are 
  highly similar and we're exploring tactics, we should perform the same tactic
  on both of them.

- Similarity is currently looked at on a note-by-note basis. But phrases
  exhibit patterns of similarity that are not evident in simple note-by-note
  comparisons.
	- Instead of looking at overall phrase duration and penalizing the outliers,
	  we could view duration as a property of phrase. It's not that we want to
	  penalize outliers from the mean; it's more that we want to penalize 
	  phrases which bear no similarity (including duration) to others.
	- What kinds of properties do phrases have? Overall duration, overall 
	  contour/shape, weighted pitch class set, duration set, 1st order markov
	  stats, surprise

- Change the way similarity factors into scoring. Right now, the algorithm 
  is filtering low similarity scores and summing the remainder. That means 
  that a phrase candidate with mediocre similarity to every other phrase is
  treated just as high as a phrase with above-average similarity to exactly
  one other phrase.
 
################################################################################
# Meter detection
################################################################################

- This article http://www.cs.tut.fi/sgn/arg/klap/sapmeter.pdf makes the point 
  that it's note-to-note "phenomenal accent" that is important. That should be
  what we're looking at in detecting meter. Sure, we can then go look back at 
  time lags for periodicity. But in meter (vs. phrase) detection, what we're 
  looking for isn't time-lagged similarity, but coincidence (at a time lag) of
  phenomenal accent.

- The same article http://www.cs.tut.fi/sgn/arg/klap/sapmeter.pdf says that 
  several meter detection algorithms used simple onset autocorrelation to detect
  subbeat and beat, but that measure detection was measured by looking for
  chord change or certain drum sounds.

- It's remarkable to me that both meter detection and phrase detection are 
  relying heavily on BSMs, yet don't share information, and rely on totally 
  separate algorithms. They both depend on structure, and both seem likely to 
  benefit from a more sophisticated analysis of the BSM.
	- what do squares mean?
	- what do think vert/horiz stripes mean?

